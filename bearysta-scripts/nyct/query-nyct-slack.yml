#
#  This is statistics aggregation recipe for Bearysta.
#
#  Install Bearysta:
#     python -m pip install git+https://github.com/IntelPython/bearysta.git
#
#  Run aggregation:
#     python -m bearysta.aggregate tpch-query-times.yml -P
#

input:
    # direct logs are in 'data/mapd_log/omnisci_server.INFO.*.log'
    path: runs/*/run-omnisci-slack/*/*.out
    format: csv
    csv-header: 'op,logID,queryID,execution_time_ms,total_time_ms'
    # Transform lines from log text into csv format. Drop other, unused lines
    filter:
        '^(?!.+ ? ([0-9]+).+ stdlog)': append
        '^.+ ? ([0-9]+).+ stdlog sql_execute ([0-9]+).+,"(\d+)","(\d+)"\}': 'sql_execute,\1,\2,\3,\4'
        '^(?!sql_)': drop

rename:
    OMP_NUM_THREADS: threads

# Aggregation method (e.g. min, median, max, mean)
aggregation: median

# Axis and series column names
axis:
    - query
    - threads
    - slack
#   - cpus
#    - affinity

series:
     - subfr
     - columnar
     - initgr
     - env
#    - branch
#    - numa

values:
#    - GB_s
    - execution_time_ms
    - total_time_ms
#    - compilation
#    - collectAllDeviceResults
#    - sort

# Create another table (or Excel filter) for each value in these columns
variants:
    - hostname
#    - scale

#precomputed:
    #query: "read_csv('tpc-h-queryID.csv', header=None, index_col=0, squeeze=True).to_dict().get(row['queryID'], row['queryID'])"
    # scale * total_size(M) / ms / 1G(1`000`000`000)
    #GB_s: "row['scale']*read_csv('tpc-h-bytes4scale.csv', header=None, index_col=0, squeeze=True).to_dict().get(row['query'], -1)/row['execution_time_ms']"
    #branch: "row['omnisci_server'].split('-')[1].split('/')[0]"

# Are higher values better?
higher-is-better: false

# Number format in Python str.format() style, or the number of digits of
# precision to show in numbers in pretty-printed pivot tables
number-format: 2

# Do we figure out the number of decimals only once, using the max value,
# and disregard smaller precision numbers?
number-format-max-only: false
